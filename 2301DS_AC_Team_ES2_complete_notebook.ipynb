{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51001373",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T09:24:53.643384Z",
     "start_time": "2021-06-11T09:24:53.622385Z"
    }
   },
   "source": [
    "# Regression Predict Student Solution\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "### Honour Code\n",
    "\n",
    "I {**2301AC_TEAM_ES2**}, confirm - by submitting this document - that the solutions in this notebook are a result of our own work and that we abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "### Predict Overview: Spain Electricity Shortfall Challenge\n",
    "\n",
    "The government of Spain is considering an expansion of it's renewable energy resource infrastructure investments. As such, they require information on the trends and patterns of the countries renewable sources and fossil fuel energy generation. Your company has been awarded the contract to:\n",
    "\n",
    "- 1. analyse the supplied data;\n",
    "- 2. identify potential errors in the data and clean the existing data set;\n",
    "- 3. determine if additional features can be added to enrich the data set;\n",
    "- 4. build a model that is capable of forecasting the three hourly demand shortfalls;\n",
    "- 5. evaluate the accuracy of the best machine learning model;\n",
    "- 6. determine what features were most important in the model’s prediction decision, and\n",
    "- 7. explain the inner working of the model to a non-technical audience.\n",
    "\n",
    "Formally the problem statement was given to you, the senior data scientist, by your manager via email reads as follow:\n",
    "\n",
    "> In this project you are tasked to model the shortfall between the energy generated by means of fossil fuels and various renewable sources - for the country of Spain. The daily shortfall, which will be referred to as the target variable, will be modelled as a function of various city-specific weather features such as `pressure`, `wind speed`, `humidity`, etc. As with all data science projects, the provided features are rarely adequate predictors of the target variable. As such, you are required to perform feature engineering to ensure that you will be able to accurately model Spain's three hourly shortfalls.\n",
    " \n",
    "On top of this, she has provided you with a starter notebook containing vague explanations of what the main outcomes are.\n",
    "\n",
    "\n",
    "## **Team_ES2**:\n",
    "#### **Members:**\n",
    "1. Onyekachukwu Ekesi ( @Onyeka_Ekesi_AC ) - email: onyekaekesi@gmail.com\n",
    "2. Mmatlou Matlakala ( @Mmatlou_Matlakala_AC ) - email: mati.matlakala@gmail.com\n",
    "3. Ibrahim Olasupo ( @Ibrahim_Olasupo_AC ) - email: ibrahim.olasupo@aiesec.net\n",
    "4. Al-Moaruf Olukayode Ajasa ( @Al_Moaruf_Ajasa_AC ) - email: ajasamoruf@gmail.com\n",
    "5. Emmanuel Osayande ( @Emmanuel_Osayande_AC ) - email: emmyfocus12@gmail.com\n",
    "6. Samuel Olaniyi ( @Samuel-Olaniyi_AC ) - email: Olusegunharshur@gmail.com\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad98ea1",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512c76c7",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "\n",
    "The constant unavailability of energy/power creates negative impacts on businesses. Ultimately, economies suffer from lack of reliable power supply.\n",
    "\n",
    "There has been a shift towards the usage of renewable energy resources globally. This is due to the ongoing campaigns against climate change and global warming. Renewable energy sources ensure a steady supply of power, hence creating new markets, enterprises, and job opportunities for the growing population. \n",
    "\n",
    "Renewable energy sources accounted for 51% of all electricity produced in Spain in the year 2022. As a result, the government is considering increasing infrastructure spending. However, they will need information on the country's renewable resource and fossil fuel energy generating trends and patterns to do so.\n",
    "\n",
    "Spain is undoubtedly a renewable energy superpower and one of the countries in the EU with the most hours of sunshine, averaging around 2500 hours per year and a radiation of kWh/m2. For that reason, electricity generation through solar panels has great potential.\n",
    "\n",
    "As consultants to the Spanish government, we were tasked with modeling the daily load shortfall between the energy generated by means of fossil fuels and various renewable sources. This will help the government sustain its target of 74% of electricity generation from renewable energy sources, notably wind and solar, by the year 2030."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817172de",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "Our objectives are:\n",
    "\n",
    "1. to explore and visualize the dataset.\n",
    "2. to clean and engineer the dataset.\n",
    "3. to build several models that will predict the daily three hourly load shortfall.\n",
    "4. to assess the accuracies of the models.\n",
    "5. to choose the best model to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab34489e",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "The various features to consider are as follow:\n",
    "\n",
    "1. Time: Weather conditions in each city at a particular date and time\n",
    "2. Wind_speed: Wind speed in each city\n",
    "3. Wind_deg: The direction of the wind in each city\n",
    "4. Pressure: Atmospheric pressure in each city\n",
    "5. Rain: The amount of rain in each city in an hour and 3 hours\n",
    "6. Snow: The amount of snowfall in each city\n",
    "7. Cloud_all: Percentage cloud coverage in each city"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9e9ee0",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to import, and briefly discuss, the libraries that will be used throughout your analysis and modelling. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a924a1a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T10:30:53.800892Z",
     "start_time": "2021-06-23T10:30:50.215449Z"
    },
    "execution": {
     "iopub.execute_input": "2023-05-15T23:47:31.053693Z",
     "iopub.status.busy": "2023-05-15T23:47:31.053242Z",
     "iopub.status.idle": "2023-05-15T23:47:31.072963Z",
     "shell.execute_reply": "2023-05-15T23:47:31.071259Z",
     "shell.execute_reply.started": "2023-05-15T23:47:31.053663Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import compress\n",
    "import plotly.express as px\n",
    "from statsmodels.graphics.correlation import plot_corr\n",
    "\n",
    "\n",
    "\n",
    "# Libraries for data preparation and model building\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "import math\n",
    "from statsmodels.graphics.correlation import plot_corr\n",
    "import statsmodels.formula.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Setting global constants to ensure notebook results are reproducible\n",
    "#PARAMETER_CONSTANT\n",
    "import random as rn\n",
    "import os\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903a02fc",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to load the data from the `df_train` file into a DataFrame. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20af9743",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:35.311495Z",
     "start_time": "2021-06-28T08:49:35.295494Z"
    },
    "execution": {
     "iopub.execute_input": "2023-05-15T23:47:31.115115Z",
     "iopub.status.busy": "2023-05-15T23:47:31.114226Z",
     "iopub.status.idle": "2023-05-15T23:47:31.279227Z",
     "shell.execute_reply": "2023-05-15T23:47:31.277595Z",
     "shell.execute_reply.started": "2023-05-15T23:47:31.115074Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "df_train = pd.read_csv(r\"df_train.csv\")\n",
    "df_test = pd.read_csv(r\"df_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e82b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-15T23:47:31.358815Z",
     "iopub.status.busy": "2023-05-15T23:47:31.358028Z",
     "iopub.status.idle": "2023-05-15T23:47:31.364237Z",
     "shell.execute_reply": "2023-05-15T23:47:31.362880Z",
     "shell.execute_reply.started": "2023-05-15T23:47:31.358767Z"
    }
   },
   "outputs": [],
   "source": [
    "# View the first 2 rows of the train DataFrame\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b164543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the dataframe structure, this shows we have 8763 rows. 49 columns\n",
    "#overview dataset\n",
    "print(f' There are {df_train.shape[0]} rows and {df_train.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6062017e",
   "metadata": {},
   "source": [
    "**From the data above, the train dataset shows we have  8,763 Observation, 48 features and 1 target variable : load_shortfall_3h**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eb0e36",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to perform an in-depth analysis of all the variables in the DataFrame. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0859404d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View the first 5 rows of the train DataFrame\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f14ea2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:52:37.824204Z",
     "start_time": "2021-06-28T08:52:37.811206Z"
    },
    "execution": {
     "iopub.execute_input": "2023-05-15T23:47:31.754060Z",
     "iopub.status.busy": "2023-05-15T23:47:31.753593Z",
     "iopub.status.idle": "2023-05-15T23:47:31.954764Z",
     "shell.execute_reply": "2023-05-15T23:47:31.953621Z",
     "shell.execute_reply.started": "2023-05-15T23:47:31.754017Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# looking at the first 5 rows above, the data was transposed below as we need to view all the columns.\n",
    "df_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d728e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the dataframe structure, this shows we have 8763 rows. 49 columns\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce249c33",
   "metadata": {},
   "source": [
    "## Observations:¶\n",
    "\n",
    "From above, we noticed the following:\n",
    "1. There were 49 columns and 8763 rows.\n",
    "2. We have an unnamed column having the same index value as seen above, this column is insignificant to our use case\n",
    "3. Valencia_wind_deg and Seville_pressure columns show category values, these are regression (numeric) data, which we need to convert to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c191f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the dataframe features\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c23c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of missing values in the dataset\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf70aae",
   "metadata": {},
   "source": [
    "## We confirmed from above that: \n",
    "\n",
    "1. The Valencia_pressure has 2068 null values, which means there were missing values. We hope to fill up the missing values using either mean or median in the featutre engineering section below.\n",
    "\n",
    "2. The datatype 'object' indicates that the columns: time, Valencia wind deg, and Seville pressure are non-numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608c6bd",
   "metadata": {},
   "source": [
    "## Descriptive Statistics of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f83b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at Data Statistics\n",
    "df_train.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa8efd9",
   "metadata": {},
   "source": [
    "#### From the data statistics above, the followings were observed:\n",
    "\n",
    "1. The Mean of the columns Barcelona Pressure, Bilbao Pressure, Valencia Pressure etc. show very large values which are far from the range.\n",
    "2. These were evident also in their maximum values which presumed that there are outliers. \n",
    "\n",
    "#### Skewness and Outliers\n",
    "\n",
    "Skew indicates how symmetrical the data is. Values greater than 1 mean high positive skew, less than -1 mean high negative skew. As such, skewness is simply the measure of symmetry or more precisely, the lack of symmetry.\n",
    "\n",
    "Kurtosis measures outliers in the data. A value greater than 3 means there is a large number of outliers, lower than 3 means there are no outliers. Kurtosis is the measure of how heavy its tails are compared to a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741d9ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skew indicates how symmetrical the data are, values greater than 1 means high positive skew, less than -1 high negative skew\n",
    "df_train.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1754a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for outliers in the different columns\n",
    "# plt.figure(figsize = [10,5])\n",
    "# df_train.skew(axis=0, skipna=True).plot()\n",
    "\n",
    "skewness = df_train.skew(axis=0, skipna=True)\n",
    "plt.figure(figsize=[15, 5])\n",
    "skewness.plot(kind='bar')\n",
    "plt.xlabel('Observations')\n",
    "plt.ylabel('Skewness')\n",
    "plt.title('Skewness of our Observations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48960b9",
   "metadata": {},
   "source": [
    "## Discussions from the skew descriptions above\n",
    "\n",
    "1. The following features have high positive symmetrical data; Bilbao_snow_3h, Barcelona_pressure, Seville_rain_3h, Barcelona_rain_3h and Valencia_snow_3h.\n",
    "\n",
    "2. Madrid_weather_id, Barcelona_weather_id and Seville_weather_id have data with high negative symmetry.It was also noted that these features are also identified as outliers.\n",
    "\n",
    "3. We have an abnormal distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measure of outliers in the data, a value greater than 3 means large number outliers, lower than 3 means no outliers\n",
    "df_train.kurtosis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e71957",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.kurtosis().plot()\n",
    "plt.savefig('plot_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1345e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kurtosis = df_train.kurtosis()\n",
    "plt.figure(figsize=[10, 5])\n",
    "kurtosis.plot(kind='bar')\n",
    "plt.xlabel('Observations')\n",
    "plt.ylabel('Kurtosis')\n",
    "plt.title('Kurtosis of our observation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead8e4c",
   "metadata": {},
   "source": [
    "## Discussions of observed Kurtosis of features\n",
    "\n",
    "Below are features with large numbers of outliers as shown by kurtosis > 3;\n",
    "\n",
    "1. Bilbao_rain_1h, Valencia_wind_speed, Barcelona_rain_1h, Seville_rain_1h, Bilbao_snow_3h, Barcelona_pressure,  Seville_rain_3h, Valencia_snow_3h, Barcelona_rain_3h, Madrid_weather_id, Barcelona_weather_id and Seville_weather_id.\n",
    "\n",
    "2. The outliers observed in Barcelona_pressure are definitely due to some sort of errors, as a pressure of 3687.564230 is too high. The maximum pressure recorded in history was 1084, and the maximum pressure for the other cities in the dataset are also below this value. This value can be replaced or dropped during data engineering.\n",
    "\n",
    "3. Valencia_wind_speed has a maximum of 52, we are of the opinion that this value was wrong as the highest wind speed recorded in history was 20.This value will also be replaced or dropped.\n",
    "\n",
    "4. The outliers in the other features maybe attributed to significant changes in weather conditions on those days that the data were collected, hence we neglet them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb60ca",
   "metadata": {},
   "source": [
    "## Visualisations of some obvious outlier features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef7a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df_train['Barcelona_pressure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922611b2",
   "metadata": {},
   "source": [
    "The boxplot shows that Barcelona_pressure has about seven outliers, we then looked at values that are greater than the maximum pressure ever recorded (1084)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the Valencia_wind_speed data\n",
    "sns.boxplot(x='Valencia_wind_speed', data=df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9df95",
   "metadata": {},
   "source": [
    "We can see from above that there are significant numbers of outliers in Valencia_wind_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04535c6",
   "metadata": {},
   "source": [
    "### An Overview of our Target Variable with Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f3929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ploting time against Load_shortfall_3h to see relationship\n",
    "fig = px.line(df_train, y = df_train['load_shortfall_3h'], x =df_train['time'], width =900, height=400 )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842137d",
   "metadata": {},
   "source": [
    "It was observed from the image above that, there was seasonality in the time axis on the load_shortfall_3h values, \n",
    "We shall break down the time feature to get a better understanding of the graph with season.\n",
    "\n",
    "To do this we breakdown the time feature into:\n",
    "\n",
    "1.Year\n",
    "2.Months\n",
    "3.Weeks\n",
    "4.Days\n",
    "5.Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55707c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['time'] = pd.to_datetime(df_train['time'])  # Convert 'time' column to datetime if it's not already\n",
    "grouped_data = df_train.groupby(df_train['time'].dt.hour)['load_shortfall_3h'].sum()\n",
    "\n",
    "plt.figure(figsize=[10, 5])\n",
    "grouped_data.plot(legend=True)\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Sum of load_shortfall_3h')\n",
    "plt.title('Sum of load_shortfall_3h by Hour of the Day')\n",
    "plt.show()\n",
    "plt.savefig('plot_image.png')\n",
    "\n",
    "'''This plot can help us visualize the variation of the sum of load_shortfall_3h throughout the day,\n",
    "    potentially providing insights into any hourly patterns or trends in the data.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec919cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_train.groupby([df_train['time'].astype('datetime64').dt.year])['load_shortfall_3h'].mean(),\n",
    "        title = 'Load_shortfall_3h grouped by Year',\n",
    "        y='load_shortfall_3h',width =800, height=400 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313de0e2",
   "metadata": {},
   "source": [
    "The yearly Load_short_fall plots indicates an increase in load short fall from 2016 down to 2017 surpassing the previous years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_train.groupby([df_train['time'].astype('datetime64').dt.month])['load_shortfall_3h'].mean(),\n",
    "        title = 'Load_shortfall_3h by Month of Year',\n",
    "        y='load_shortfall_3h', width =800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a3a51",
   "metadata": {},
   "source": [
    "Also the plot above, indicates a higher 'load short fall' from June to December"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51722ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_train.groupby([df_train['time'].astype('datetime64').dt.weekofyear])['load_shortfall_3h'].mean(), \n",
    "        title = 'Load_shortfall_3h by Week of the Year', y='load_shortfall_3h', width =700, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3a1af1",
   "metadata": {},
   "source": [
    "There is no much information that can be deduced from the week of the year Load_short_fall as shown above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573072c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_train.groupby([df_train['time'].astype('datetime64').dt.dayofyear])['load_shortfall_3h'].mean(), \n",
    "        title = 'Load_shortfall_3h by Day of the Year', y='load_shortfall_3h', width =700, height=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07beb4a",
   "metadata": {},
   "source": [
    "The minimum load_short_fall_3h recorded was 1,862k while the maximum was 17,306k as indicated from the Day of the year plot above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209925b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_train.groupby([df_train['time'].astype('datetime64').dt.day])['load_shortfall_3h'].mean(), \n",
    "        title = 'Load_shortfall_3h by Day of the Month', y='load_shortfall_3h', width =800, height=400 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ec77b2",
   "metadata": {},
   "source": [
    "The plots above shows 10k to 12k consistent recorded values from the middle to the end of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50968733",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_train.groupby([df_train['time'].astype('datetime64').dt.dayofweek])['load_shortfall_3h'].mean(), \n",
    "        title = 'Load_shortfall_3h by Day of the Week', y='load_shortfall_3h', width =800, height=400 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578e03d1",
   "metadata": {},
   "source": [
    "We noticed from above that, there was decline in the Load_short_fall_3h Day of the week plots on from Thursday to Saturdays, we can't substantiate the reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d1adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_train.groupby([df_train['time'].astype('datetime64').dt.hour])['load_shortfall_3h'].mean(), \n",
    "        title = 'Load_shortfall_3h by Hour of Day', y='load_shortfall_3h', width =800, height=400 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d4dc29",
   "metadata": {},
   "source": [
    "There seems to be an increase in the Load_short_fall_3h hourly plots each day, mostly from 10 hours and above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4466c467",
   "metadata": {},
   "source": [
    "# Cloud levels examination\n",
    "Examining the Cloud Conditions for the 3 Given Cities (namely: Madrid, Bilbao and Seville)  is pertinent and to making concrete decisions concerning the type of enegy source to recommend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf05f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_train.groupby([df_train['time'].astype('datetime64').dt.dayofyear])['Madrid_temp_max',].mean(), \n",
    "        title = 'Madrid Maximum temperature by 365 days in a year', y='Madrid_temp_max', width =800, height=400 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ffc38af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "px.line(df_train.groupby([df_train['time'].astype('datetime64').dt.dayofyear])['Madrid_clouds_all',].mean(), \n",
    "        title = 'Madrid cloud by 365 days in a year', y='Madrid_clouds_all', width =800, height=400 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe211aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_train.groupby([df_train['time'].astype('datetime64').dt.dayofyear])['Bilbao_clouds_all',].mean(), \n",
    "        title = 'Bilbao cloud by 365 days in a year', y='Bilbao_clouds_all', width =800, height=400 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8416b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_train.groupby([df_train['time'].astype('datetime64').dt.dayofyear])['Seville_clouds_all',].mean(), \n",
    "        title = 'Seville cloud by 365 days in a year', y='Seville_clouds_all', width =800, height=400 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42478c97",
   "metadata": {},
   "source": [
    "From the above cloud plots, Bilbao has a high cloud level which makes it a city that may thrive using wind energy, while Madrid and Seville cloud pattern is negatively correlated with their temperature pattern. this makes Solar energy a better option in Madrid, Seville, Valencia and Barcelona. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e86186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.plot(x='time',y='Madrid_temp_max',fontsize = 10, figsize =(10,6))\n",
    "plt.title('Temperatures over time',fontsize=16)\n",
    "plt.xlabel('time', fontsize=16)\n",
    "plt.ylabel('Temperatures', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77985941",
   "metadata": {},
   "source": [
    "From above we can conveniently see that the temperatures change seasonally. This suggested other feautres, such as hours, days, weeks, and months?\n",
    "We check same for the whole five cities below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2664ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.plot(x='time',y=['Seville_temp_max','Bilbao_temp_max','Barcelona_temp_max','Valencia_temp_max','Madrid_temp_max'],fontsize = 10,\n",
    "ylabel = 'Temp Kelvin', figsize =(10,6),alpha=0.8)\n",
    "plt.title(\"Temperatures over time for all 5 cities\",fontsize=14)\n",
    "plt.xlabel('Time', fontsize=8)\n",
    "plt.ylabel('Temperature Kelvin', fontsize=10)\n",
    "plt.savefig('plot_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f61e19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.plot(x='time',y=['Seville_wind_speed','Bilbao_wind_speed','Barcelona_wind_speed','Valencia_wind_speed','Madrid_wind_speed'],fontsize = 10,\n",
    "ylabel = 'meters per second', figsize =(10,6),alpha=0.8)\n",
    "plt.title(\"Wind speed over time for all 5 cities\",fontsize=14)\n",
    "plt.xlabel('Time', fontsize=8)\n",
    "plt.ylabel('Wind speed', fontsize=10)\n",
    "plt.savefig('plot_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.plot(x='time',y=['Seville_clouds_all','Bilbao_clouds_all','Madrid_clouds_all',],fontsize = 10,\n",
    "ylabel = '', figsize =(10,6),alpha=0.8)\n",
    "plt.title(\"cloud over time for 3 given cities\",fontsize=14)\n",
    "plt.xlabel('Time', fontsize=8)\n",
    "plt.ylabel('cloud', fontsize=10)\n",
    "plt.savefig('plot_image.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f0c44a",
   "metadata": {},
   "source": [
    "We observed near uniform range in temperature across all the cities and the trend seems normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(df_train['load_shortfall_3h'])\n",
    "\n",
    "plt.figure(figsize=[10, 5])\n",
    "plt.hist(df_train['load_shortfall_3h'], bins=10)  # Adjust the number of bins as needed\n",
    "plt.xlabel('Load Shortfall (3-hour)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Load Shortfall (3-hour)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcffc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the correlation\n",
    "fig = plt.figure(figsize=(10,8));\n",
    "ax = fig.add_subplot(111);\n",
    "plot_corr(df_train.corr(), xnames = df_train.corr().columns, ax = ax, );\n",
    "plt.savefig('plot_image.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4598ebe",
   "metadata": {},
   "source": [
    "### Observations from the correllation heatmap visualisation above\n",
    "\n",
    "We noticed high presence of high correlations (the wine red) between features on the heatmap at the bottom right corner of our heatmap.\n",
    "It is important to consider this step when choosing the best features which in turn would result to an improvement of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b33e3ba",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to: clean the dataset, and possibly create new features - as identified in the EDA phase. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0461b7",
   "metadata": {},
   "source": [
    "### We have highlighted some key points to consider -\n",
    "\n",
    "\n",
    "1. Feature Selection/Importance\n",
    "2. Handling missing values\n",
    "3. Handling outliers\n",
    "4. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "194bf6a3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-15T23:47:40.341697Z",
     "iopub.status.idle": "2023-05-15T23:47:40.342286Z",
     "shell.execute_reply": "2023-05-15T23:47:40.342014Z",
     "shell.execute_reply.started": "2023-05-15T23:47:40.341986Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove missing values/ features\n",
    "\n",
    "df_train['Valencia_pressure'].fillna(df_train['Valencia_pressure'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d512d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"First we will start by Transforming the Valencia_wind_deg and Seville_pressure columns to numeric.\n",
    "This is done to ensure uniformity (numeric values) in our model\"\"\"\n",
    "\n",
    "df_train['Valencia_wind_deg'] = df_train['Valencia_wind_deg'].str.extract('(\\d+)').astype('int64')\n",
    "df_train['Seville_pressure'] = df_train['Seville_pressure'].str.extract('(\\d+)').astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3146fd4",
   "metadata": {},
   "source": [
    "The next step is to engineer new features from the time column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba0d62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new features\n",
    "\n",
    "df_train['Year']  = df_train['time'].astype('datetime64').dt.year\n",
    "df_train['Month_of_year']  = df_train['time'].astype('datetime64').dt.month\n",
    "df_train['Week_of_year'] = df_train['time'].astype('datetime64').dt.weekofyear\n",
    "df_train['Day_of_year']  = df_train['time'].astype('datetime64').dt.dayofyear\n",
    "df_train['Day_of_month']  = df_train['time'].astype('datetime64').dt.day\n",
    "df_train['Day_of_week'] = df_train['time'].astype('datetime64').dt.dayofweek\n",
    "df_train['Hour_of_week'] = ((df_train['time'].astype('datetime64').dt.dayofweek) * 24 + 24) - (24 - df_train['time'].astype('datetime64').dt.hour)\n",
    "df_train['Hour_of_day']  = df_train['time'].astype('datetime64').dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da127a23",
   "metadata": {},
   "source": [
    "Let us have a look at the correlation(s) between our newly created temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce7452",
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_df = df_train.iloc[:,[-8,-7,-6,-5,-4,-3,-2,-1]]\n",
    "plt.figure(figsize=[10,6])\n",
    "sns.heatmap(Time_df.corr(),annot=True )\n",
    "plt.title('features')\n",
    "plt.savefig('features.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e19590",
   "metadata": {},
   "source": [
    "From the heatmap correllation above,we have high Multicollinearity present in our new features. The features involved are -\n",
    "\n",
    "1.Week of the year,\n",
    "2.Day of the year,\n",
    "3.Month of the year,\n",
    "4.Day of the week,\n",
    "5.Hour of the week,\n",
    "\n",
    "We would have to drop either one of the features that have high correlation with each other\n",
    "\n",
    "Alongside dropping these features mentioned above, we would also be dropping the time and Unnamed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ceac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=['Week_of_year','Day_of_year','Hour_of_week', 'Unnamed: 0','time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc939887",
   "metadata": {},
   "source": [
    "## Feature Importance/Selection\n",
    "\n",
    "Feature selection is the process where you automatically or manually select the features that contribute the most to your prediction variable or output. Selecting the important independent features which have more relation with the dependent feature will help to build a good model. There are some methods for feature selection:\n",
    "\n",
    "Feature importance gives you a score for each feature of your data. The higher the score, the more important or relevant that feature is to your target feature.\n",
    "\n",
    "###### Feature importance is an inbuilt class that comes with tree-based classifiers such as:\n",
    "\n",
    "1. Random Forest Classifiers.\n",
    "2. Extra Tree Classifiers.\n",
    "\n",
    "\n",
    "###### Correlation Matrix with Heatmap\n",
    "\n",
    "Heatmap is a graphical representation of 2D (two-dimensional) data. Each data value is represented in a matrix.\n",
    "\n",
    "First, we'll plot the pair plot between all independent features and dependent features. It will give the relation between dependent and independent features. If the relations between the independent feature and the dependent feature is less than 0.2, we then choose that independent feature for building a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[35,15])\n",
    "sns.heatmap(df_train.corr(),annot=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce2e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[35, 15])\n",
    "sns.heatmap(df_train.corr(), annot=True)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b23597d",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to create one or more regression models that are able to accurately predict the thee hour load shortfall. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5ac405",
   "metadata": {},
   "source": [
    "Just as we mentioned in our EDA, we noticed the presence of high correlations between the predictor columns and also possible outliers.\n",
    "\n",
    "Here, we would have to drop these columns to improve the performance of our model and reduce any possibility of overfitting in our model\n",
    "\n",
    "Before we drop these columns, we need to fill the missing values\n",
    "\n",
    "**Filling Missing Values**\n",
    "\n",
    "To do this, we will fill in the missing values present in Valencia_pressure column using the mean method. Let us check if this approach corresponds with our feature selection.\n",
    "\n",
    "Using SelectKBest and Chi2 to perform Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting our data into dependent Variable and Independent Variable\n",
    "X = df_train.drop(columns = 'load_shortfall_3h')\n",
    "y = df_train['load_shortfall_3h'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9409a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "featureScores = pd.concat([dfcolumns, dfscores], axis=1)\n",
    "featureScores.columns = ['Features', 'Score']\n",
    "new_X = featureScores.sort_values('Score',ascending=False).head(40)\n",
    "new_X.tail(10) #To get the least important feature based on their score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ef927",
   "metadata": {},
   "source": [
    "This result confirms our claim, where we noticed in the heatmap multicollinearity between features, and from our feature selection, we can see those features as having the lowest significance in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9601c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-15T23:59:46.466285Z",
     "iopub.status.busy": "2023-05-15T23:59:46.465878Z",
     "iopub.status.idle": "2023-05-15T23:59:46.482862Z",
     "shell.execute_reply": "2023-05-15T23:59:46.481681Z",
     "shell.execute_reply.started": "2023-05-15T23:59:46.466253Z"
    }
   },
   "source": [
    "**Dropping Outliers**\n",
    "\n",
    "We have to remove possible outliers and select the important features for our model, thus dropping others having multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138d14dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[['Madrid_wind_speed', 'Valencia_wind_deg', 'Bilbao_rain_1h',\n",
    "       'Valencia_wind_speed', 'Seville_humidity', 'Madrid_humidity',\n",
    "       'Bilbao_clouds_all', 'Bilbao_wind_speed', 'Seville_clouds_all',\n",
    "       'Bilbao_wind_deg', 'Barcelona_wind_speed', 'Barcelona_wind_deg',\n",
    "       'Madrid_clouds_all', 'Seville_wind_speed', 'Barcelona_rain_1h',\n",
    "       'Seville_pressure', 'Seville_rain_1h', 'Bilbao_snow_3h',\n",
    "       'Barcelona_pressure', 'Seville_rain_3h', 'Madrid_rain_1h',\n",
    "       'Barcelona_rain_3h', 'Valencia_snow_3h', 'Madrid_weather_id',\n",
    "       'Barcelona_weather_id', 'Bilbao_pressure', 'Seville_weather_id',\n",
    "       'Valencia_pressure', 'Seville_temp_max', 'Bilbao_weather_id', \n",
    "        'Valencia_humidity', 'Year', 'Month_of_year', 'Day_of_month', 'Day_of_week', 'Hour_of_day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeffd0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20,10])\n",
    "sns.heatmap(X.corr(),annot=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14714b53",
   "metadata": {},
   "source": [
    "We have been able to remove the collinearity seen in previous heatmaps and also selected specific features to train our model with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29d27cc",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Lastly,it is important to scale our data before we model at all. As we noticed during the EDA, some features had values that were out of range when we compared their mean, max and standard deviation. This can result to bias in the model during decision making, thus it is important to convert all the column values to a certain range/scale.\n",
    "\n",
    "**What is Feature Scaling?**\n",
    "\n",
    "Feature scaling is the process of normalising the range of features in a dataset. Real-world datasets often contain features that are varying in degrees of magnitude, range and units. Therefore, in order for machine learning models to interpret these features on the same scale, we need to perform feature scaling.\n",
    "\n",
    "We are going to use Standard Scaling, becasue of it's robustness to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a87fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standardization object\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22d1a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save standardized features into new variable\n",
    "\"\"\"\n",
    "We used a fit transform method, which first fits in the standardscaler and then transforms the data \"\"\"\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled,columns=X.columns)\n",
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d65ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866132cb",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "1. We are going to split the data into train and test, to be able to evaluate the model that we build on the train data.\n",
    "2. Build a Linear Regression model which would serve as our base model using the train data.\n",
    "3. Try and improve the linear model by employing Lasso and Ridge\n",
    "4. Try out other models like decision trees, Random Forest and SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1899b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating our models into training set and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f42b6f",
   "metadata": {},
   "source": [
    "What we did above was to split our data into 80% for training and the remaining 20% for testing i.e test_size = 0.2. We made use of the train_test_split syntax from the sklearn library to carry out the splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape of the training and testing data\n",
    "\n",
    "print('Training predictor:', X_train.shape)\n",
    "print('Training target:', y_train.shape)\n",
    "print('Testing predictor:', X_test.shape)\n",
    "print('Testing target:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbdbe70",
   "metadata": {},
   "source": [
    "From the above, we can observe that 7010 features were allocated to the training set and 1753 to testing set of our data\n",
    "\n",
    "N/B: These values are as a result of the splitting ratio carried out, it is important to note that any change in the splitting \n",
    "ratio would affect the value and shape of the training and testing sets in Multiple linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cecd2b",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 6. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model performance ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9390fd",
   "metadata": {},
   "source": [
    "As our baseline, we would first make use of Linear Model.\n",
    "The term linear model implies that the model is specified as a linear combination of features. Based on training data, the learning process computes one weight for each feature to form a model that can predict or estimate the target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc5591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the model\n",
    "lm = LinearRegression()\n",
    "#Fit the model into training set\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "#predict on unseen data\n",
    "predict = lm.predict(X_test)\n",
    "train_predict = lm.predict(X_train) #predicting on the same training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a95e12",
   "metadata": {},
   "source": [
    "## Lasso Regression (L1 Norm)\n",
    "\n",
    "Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters).\n",
    "\n",
    "The lasso regression allows you to shrink or regularize these coefficients to avoid overfitting and make them work better on different datasets. This type of regression is used when the dataset shows high multicollinearity or when you want to automate variable elimination and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef1f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LASSO model object, setting alpha to 0.01\n",
    "\"\"\" when alpha is 0, Lasso regression produces the same coefficients as a linear regression. When alpha is very very large, all coefficients are zero.\"\"\"\n",
    "lasso = Lasso(alpha=0.01)\n",
    "# Train the LASSO model\n",
    "lasso.fit(X_train, y_train)\n",
    "# Get predictions\n",
    "lasso_pred = lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0ed10",
   "metadata": {},
   "source": [
    "### Ridge Regression (L2 Norm)\n",
    "\n",
    "Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf0a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Ridge model\n",
    "Ridge = Ridge()\n",
    "# Train the model\n",
    "Ridge.fit(X_train, y_train)\n",
    "# Get predictions\n",
    "Ridge_pred = Ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132d174c",
   "metadata": {},
   "source": [
    "Support Vector Regressor\n",
    "While linear regression models minimize the error between the actual and predicted values through the line of best fit, SVR manages to fit the best line within a threshold of values.\n",
    "\n",
    "SVR uses the same basic idea as Support Vector Machine (SVM), a classification algorithm, but applies it to predict real values rather than a class.\n",
    "The aim is to fit as many instances as possible between the lines while limiting the margin violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff52983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate support vector regression model\n",
    "Sv_reg = SVR(kernel='rbf', gamma='auto')\n",
    "# Train the model\n",
    "Sv_reg.fit(X_train,y_train)\n",
    "# Get predictions\n",
    "SV_pred = Sv_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4b7bdc",
   "metadata": {},
   "source": [
    "Decision Tree Model\n",
    "Decision tree regression observes features of an object and trains a model in the structure of a tree to predict data in the future to produce meaningful continuous output. Continuous output means that the output/result is not discrete, i.e., it is not represented just by a discrete, known set of numbers or values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f54bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate regression tree model\n",
    "Reg_tree = DecisionTreeRegressor(random_state=42)\n",
    "# Fitting the model\n",
    "Reg_tree.fit(X_train,y_train)\n",
    "Tree_pred = Reg_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e526cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    'Prediction': Tree_pred,\n",
    "    # Add any necessary identifier columns here\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission_dTree.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e43029",
   "metadata": {},
   "source": [
    "Random Forest\n",
    "Random Forest Regression is a supervised learning algorithm that uses ensemble learning method for regression. Ensemble learning method is a technique that combines predictions from multiple machine learning algorithms to make a more accurate prediction than a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1922f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our forest consists of 200 trees with a max depth of 8 \n",
    "RF = RandomForestRegressor(n_estimators=200, max_depth=8)\n",
    "# Fitting the model\n",
    "RF.fit(X_train,y_train)\n",
    "RF_predict = RF.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7b227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    'Prediction': RF_predict,})\n",
    "    \n",
    "submission_df.to_csv('submission06.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11de55a",
   "metadata": {},
   "source": [
    "**Evaluation of Models**\n",
    "\n",
    "We are going to evaluate the performance of SIX MODELS we trained using metrics such as-\n",
    "\n",
    "Root Mean Squared Error (RMSE),\n",
    "Mean Squared Error (MSE),\n",
    "Mean Absolute Error (MAE),\n",
    "Residual Sum of Squares Error (RSS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3501fea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing the True value and the Predicted Value of our models\n",
    "Linear = pd.DataFrame({'Actual': y_test, 'Predicted': predict})\n",
    "Lass_ = pd.DataFrame({'Actual': y_test, 'Predicted': lasso_pred})\n",
    "Ridge_ = pd.DataFrame({'Actual': y_test, 'Predicted': Ridge_pred})\n",
    "Sv_ = pd.DataFrame({'Actual': y_test, 'Predicted': SV_pred})\n",
    "Des_ = pd.DataFrame({'Actual': y_test, 'Predicted': Tree_pred})\n",
    "Rand_ = pd.DataFrame({'Actual': y_test, 'Predicted': RF_predict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05025aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Linear.head()) #Linear Model \n",
    "print('\\n')\n",
    "print(Lass_.head()) # Lasso Model\n",
    "print('\\n')\n",
    "print(Ridge_.head()) # Ridge Model\n",
    "print('\\n')\n",
    "print(Sv_.head()) #SVR Model\n",
    "print('\\n')\n",
    "print(Des_.head()) #Decision Tree Model\n",
    "print('\\n')\n",
    "print(Rand_.head()) # Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f263f661",
   "metadata": {},
   "source": [
    "**Discussion of the evaluation outcomes of the six models above**\n",
    "\n",
    "From the Predicted values above, we can see some models have values very close to the actual label.Some of these results might \n",
    "be attributed to overfitting and also exposed to a lot of outliers.\n",
    "\n",
    "To confirm further, we therefore, test our model's performance based on the Metrics aforementioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d51774f",
   "metadata": {},
   "source": [
    "### Comparing the Root Mean Square Error across Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60c3387",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Performance = { \n",
    "    \n",
    "                      'Test RMSE':\n",
    "                    \n",
    "                        {\"Linear model\": np.sqrt(metrics.mean_squared_error(y_test,predict)),\n",
    "                        \"Ridge\": np.sqrt(metrics.mean_squared_error(y_test,Ridge_pred)),\n",
    "                        \"Lasso\" : np.sqrt(metrics.mean_squared_error(y_test,lasso_pred)),\n",
    "                         \"SVR\" : np.sqrt(metrics.mean_squared_error(y_test,SV_pred)),\n",
    "                        \"Decision Tree\" : np.sqrt(metrics.mean_squared_error(y_test,Tree_pred)),\n",
    "                        \"Random Forest\" : np.sqrt(metrics.mean_squared_error(y_test,RF_predict))}\n",
    "                        \n",
    "                    }\n",
    "\n",
    "# create dataframe from dictionary\n",
    "Model_Performance = pd.DataFrame(data=Model_Performance)\n",
    "Model_Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e7bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(Model_Performance, y =Model_Performance['Test RMSE'],\n",
    "       color = Model_Performance.index, width =700, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87078277",
   "metadata": {},
   "source": [
    "From the graph above, we can confirm that the Random Forest model performs better than others in terms of RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4636284",
   "metadata": {},
   "source": [
    "## Comparing the Mean Square Error across Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea6b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Performance2 = { \n",
    "    \n",
    "                      'Test MSE':\n",
    "                    \n",
    "                        {\"Linear model\": (metrics.mean_squared_error(y_test,predict)),\n",
    "                        \"Ridge\": (metrics.mean_squared_error(y_test,Ridge_pred)),\n",
    "                        \"Lasso\" : (metrics.mean_squared_error(y_test,lasso_pred)),\n",
    "                         \"SVR\" : (metrics.mean_squared_error(y_test,SV_pred)),\n",
    "                        \"Decision Tree\" : (metrics.mean_squared_error(y_test,Tree_pred)),\n",
    "                        \"Random Forest\" : (metrics.mean_squared_error(y_test,RF_predict))}\n",
    "                        \n",
    "                    }\n",
    "\n",
    "# create dataframe from dictionary\n",
    "Model_Performance2 = pd.DataFrame(data=Model_Performance2)\n",
    "Model_Performance2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d401aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(Model_Performance2, y =Model_Performance2['Test MSE'],\n",
    "       color = Model_Performance2.index, width =700, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fbe42",
   "metadata": {},
   "source": [
    "From the graph above, we can confirm that the Random Forest model performs better than others in terms of MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27537dd9",
   "metadata": {},
   "source": [
    "## Comparing the Mean Absolute Error across Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9748634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Performance3= { \n",
    "    \n",
    "                      'Test MAE':\n",
    "                    \n",
    "                        {\"Linear model\": (metrics.mean_absolute_error(y_test,predict)),\n",
    "                        \"Ridge\": (metrics.mean_absolute_error(y_test,Ridge_pred)),\n",
    "                        \"Lasso\" : (metrics.mean_absolute_error(y_test,lasso_pred)),\n",
    "                         \"SVR\" : (metrics.mean_absolute_error(y_test,SV_pred)),\n",
    "                        \"Decision Tree\" : (metrics.mean_absolute_error(y_test,Tree_pred)),\n",
    "                        \"Random Forest\" : (metrics.mean_absolute_error(y_test,RF_predict))}\n",
    "                        \n",
    "                    }\n",
    "\n",
    "# create dataframe from dictionary\n",
    "Model_Performance3 = pd.DataFrame(data=Model_Performance3)\n",
    "Model_Performance3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05680114",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(Model_Performance3, y =Model_Performance3['Test MAE'],\n",
    "       color = Model_Performance3.index, width =700, height=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656c0b42",
   "metadata": {},
   "source": [
    "From the graph above, we can confirm that the Random Forest model performs better than others in terms of MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45864817",
   "metadata": {},
   "source": [
    "## Comparing the R-Squared across Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305abd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Performance4= { \n",
    "    \n",
    "                      'Test R^2':\n",
    "                    \n",
    "                        {\"Linear model\": (metrics.r2_score(y_test,predict)),\n",
    "                        \"Ridge\": (metrics.r2_score(y_test,Ridge_pred)),\n",
    "                        \"Lasso\" : (metrics.r2_score(y_test,lasso_pred)),\n",
    "                         \"SVR\" : (metrics.r2_score(y_test,SV_pred)),\n",
    "                        \"Decision Tree\" : (metrics.r2_score(y_test,Tree_pred)),\n",
    "                        \"Random Forest\" : (metrics.r2_score(y_test,RF_predict))}\n",
    "                        \n",
    "                    }\n",
    "\n",
    "# create dataframe from dictionary\n",
    "Model_Performance4 = pd.DataFrame(data=Model_Performance4)\n",
    "Model_Performance4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(Model_Performance4, y =Model_Performance4['Test R^2'],\n",
    "       color = Model_Performance4.index, width =700, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12923a7e",
   "metadata": {},
   "source": [
    "**Random Forest Performed Best**\n",
    "\n",
    "From the graph above, we can confirm that the Random Forest model performs better than others in terms of R^2.\n",
    "\n",
    "From these results, we will conclude to pick Random Forest for our model Predictions as it meets all the expectations for a \n",
    "regression model and gives better performing metric.\n",
    "\n",
    "Random Forest has a higher R2 for Test data as compared to the other models.Random Forest again has a lower RMSE for both \n",
    "the Training and Test data as compared to the other models.\n",
    "\n",
    "We can therefore conclude that Random Forest is the best model to use for prediction of 3 hourly load shortfall in Spain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c485af",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-15T23:47:40.386144Z",
     "iopub.status.idle": "2023-05-15T23:47:40.386705Z",
     "shell.execute_reply": "2023-05-15T23:47:40.386379Z",
     "shell.execute_reply.started": "2023-05-15T23:47:40.386360Z"
    }
   },
   "source": [
    "### Discussions\n",
    "\n",
    "Coefficient of determination (R2) measures the amount of variance in the predictions explained by the dataset.It is the \n",
    "difference between the samples in the dataset and the predictions made by the model.It measures from 0 to 1 with 1\n",
    "representing a perfect model and 0 showing that the model will perform badly on unseen data.\n",
    "\n",
    "RMSE is the square root of the Mean Square Error (MSE). MSE represents the average of the squared difference between the \n",
    "true and predicted values, it measures the variance of the residuals. While the RMSE measures the standard deviation of the residuals.\n",
    "\n",
    "The smaller the RMSE of the model the better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3f225b",
   "metadata": {},
   "source": [
    "## Predicting Load_shortfall_3h on our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d8be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(r\"df_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3fde2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b05aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Engineering New Features ( i.e Desampling the Time) that will help us in our modeling\n",
    "\n",
    "df_test['Year']  = df_test['time'].astype('datetime64').dt.year\n",
    "df_test['Month_of_year']  = df_test['time'].astype('datetime64').dt.month\n",
    "df_test['Week_of_year'] = df_test['time'].astype('datetime64').dt.weekofyear\n",
    "df_test['Day_of_year']  = df_test['time'].astype('datetime64').dt.dayofyear\n",
    "df_test['Day_of_month']  = df_test['time'].astype('datetime64').dt.day\n",
    "df_test['Day_of_week'] = df_test['time'].astype('datetime64').dt.dayofweek\n",
    "df_test['Hour_of_week'] = ((df_test['time'].astype('datetime64').dt.dayofweek) * 24 + 24) - (24 - df_test['time'].astype('datetime64').dt.hour)\n",
    "df_test['Hour_of_day']  = df_test['time'].astype('datetime64').dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling missing values\n",
    "time = df_test['time']\n",
    "\n",
    "df_test['Valencia_pressure'].fillna(df_test['Valencia_pressure'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dbbbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[['Madrid_wind_speed', 'Valencia_wind_deg', 'Bilbao_rain_1h',\n",
    "       'Valencia_wind_speed', 'Seville_humidity', 'Madrid_humidity',\n",
    "       'Bilbao_clouds_all', 'Bilbao_wind_speed', 'Seville_clouds_all',\n",
    "       'Bilbao_wind_deg', 'Barcelona_wind_speed', 'Barcelona_wind_deg',\n",
    "       'Madrid_clouds_all', 'Seville_wind_speed', 'Barcelona_rain_1h',\n",
    "       'Seville_pressure', 'Seville_rain_1h', 'Bilbao_snow_3h',\n",
    "       'Barcelona_pressure', 'Seville_rain_3h', 'Madrid_rain_1h',\n",
    "       'Barcelona_rain_3h', 'Valencia_snow_3h', 'Madrid_weather_id',\n",
    "       'Barcelona_weather_id', 'Bilbao_pressure', 'Seville_weather_id',\n",
    "       'Valencia_pressure', 'Seville_temp_max', 'Bilbao_weather_id', \n",
    "        'Valencia_humidity', 'Year', 'Month_of_year', 'Day_of_month', 'Day_of_week', 'Hour_of_day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f5e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming Valencia_wind_deg to numeric\n",
    "\n",
    "df_test['Valencia_wind_deg'] = df_test['Valencia_wind_deg'].str.extract('(\\d+)').astype('int64')\n",
    "df_test['Seville_pressure'] = df_test['Seville_pressure'].str.extract('(\\d+)').astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe32d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['load_shortfall_3h'] = RF.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023b49ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['time'] = time\n",
    "load = df_test[['time','load_shortfall_3h']]\n",
    "load.to_csv('submission_load_shortfall.csv', index = False)\n",
    "load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f24e3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T00:00:19.424620Z",
     "iopub.status.busy": "2023-05-16T00:00:19.424243Z",
     "iopub.status.idle": "2023-05-16T00:00:19.438130Z",
     "shell.execute_reply": "2023-05-16T00:00:19.437017Z",
     "shell.execute_reply.started": "2023-05-16T00:00:19.424589Z"
    }
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0b903a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T00:12:07.676484Z",
     "iopub.status.busy": "2023-05-16T00:12:07.676076Z",
     "iopub.status.idle": "2023-05-16T00:12:07.683652Z",
     "shell.execute_reply": "2023-05-16T00:12:07.682220Z",
     "shell.execute_reply.started": "2023-05-16T00:12:07.676452Z"
    }
   },
   "outputs": [],
   "source": [
    "# create targets and features dataset\n",
    "y = df_eda['load_shortfall_3h']\n",
    "X = df_eda['load_shortfall_3h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae5da5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T00:14:12.154359Z",
     "iopub.status.busy": "2023-05-16T00:14:12.153919Z",
     "iopub.status.idle": "2023-05-16T00:14:12.190125Z",
     "shell.execute_reply": "2023-05-16T00:14:12.188488Z",
     "shell.execute_reply.started": "2023-05-16T00:14:12.154316Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96430da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T00:00:36.801136Z",
     "iopub.status.busy": "2023-05-16T00:00:36.800768Z",
     "iopub.status.idle": "2023-05-16T00:00:36.805891Z",
     "shell.execute_reply": "2023-05-16T00:00:36.804325Z",
     "shell.execute_reply.started": "2023-05-16T00:00:36.801106Z"
    }
   },
   "outputs": [],
   "source": [
    "# create one or more ML models\n",
    "lr = LinearRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1845bb48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T00:00:49.464551Z",
     "iopub.status.busy": "2023-05-16T00:00:49.464142Z",
     "iopub.status.idle": "2023-05-16T00:00:49.513133Z",
     "shell.execute_reply": "2023-05-16T00:00:49.511380Z",
     "shell.execute_reply.started": "2023-05-16T00:00:49.464504Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit models on training data\n",
    "lr.fit(x_train, y_train)\n",
    "preds =lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb648911",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7e1e6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-15T23:47:40.377429Z",
     "iopub.status.idle": "2023-05-15T23:47:40.377840Z",
     "shell.execute_reply": "2023-05-15T23:47:40.377661Z",
     "shell.execute_reply.started": "2023-05-15T23:47:40.377643Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate one or more ML models\n",
    "\n",
    "def rmse(y_test, y_predict):\n",
    "    return np.sqrt(meana_squared_error(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e164f2c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T00:00:55.064052Z",
     "iopub.status.busy": "2023-05-16T00:00:55.063662Z",
     "iopub.status.idle": "2023-05-16T00:00:55.069200Z",
     "shell.execute_reply": "2023-05-16T00:00:55.068029Z",
     "shell.execute_reply.started": "2023-05-16T00:00:55.064021Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece2d9a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T00:01:02.522762Z",
     "iopub.status.busy": "2023-05-16T00:01:02.522151Z",
     "iopub.status.idle": "2023-05-16T00:01:02.535083Z",
     "shell.execute_reply": "2023-05-16T00:01:02.533110Z",
     "shell.execute_reply.started": "2023-05-16T00:01:02.522719Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bc9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da968e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T00:32:52.335517Z",
     "iopub.status.busy": "2023-05-16T00:32:52.335000Z",
     "iopub.status.idle": "2023-05-16T00:32:52.348366Z",
     "shell.execute_reply": "2023-05-16T00:32:52.347438Z",
     "shell.execute_reply.started": "2023-05-16T00:32:52.335472Z"
    }
   },
   "outputs": [],
   "source": [
    "daf=pd.DataFrame(preds, columns=['load_shortfall_3h'])\n",
    "daf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd82fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T00:42:58.530679Z",
     "iopub.status.busy": "2023-05-16T00:42:58.529586Z",
     "iopub.status.idle": "2023-05-16T00:42:59.004249Z",
     "shell.execute_reply": "2023-05-16T00:42:59.002930Z",
     "shell.execute_reply.started": "2023-05-16T00:42:58.530638Z"
    }
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame({\"time\": df_test['time']})\n",
    "submission = output.join(daf)\n",
    "submission= submission[['time','load_shortfall_3h']]\n",
    "submission = submission[~(submission['load_shortfall_3h'].isnull())]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e047e1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T00:43:06.271711Z",
     "iopub.status.busy": "2023-05-16T00:43:06.271295Z",
     "iopub.status.idle": "2023-05-16T00:43:06.309489Z",
     "shell.execute_reply": "2023-05-16T00:43:06.308239Z",
     "shell.execute_reply.started": "2023-05-16T00:43:06.271677Z"
    }
   },
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe5e0ff",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Model Explanations\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model explanation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172524da",
   "metadata": {},
   "source": [
    "## Discuss choosen model logic:\n",
    "\n",
    "1. In the beginning, we built our model using linear regression, though,a linear model is not appropriate for data that is not \n",
    "linear, it also suffers from multicolinearity.\n",
    "\n",
    "2. Ridge regression shrinks the coefficients and it helps to reduce the model complexity and multicollinearity.\n",
    "\n",
    "3. Lasso regression helps in reducing overfitting and in feature selection by setting coefficients with high values to zero.\n",
    "\n",
    "4. Decision trees are not affected by multicolinearity, they support non-linearity and are resistant to outliers furthermore \n",
    "they require little data preprocessing. However Decision trees are prone to overfitting and parameter tuning can led to biased \n",
    "learned trees if some classes dominate.\n",
    "\n",
    "5. Random forests address the problem of overfitting.They use ensemble learning methods for regression by constructing several \n",
    "Decision trees during training and outputs the mean of the classes as the prediction of all the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15a50e8",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "As consultants to the government of Spain,  we came out succesfully with a model that predicts the 3 hourly load shortfall of power generated by renewable energy sources in the country. This will help in informed decisions by the Spanish Government of the trends and patterns of the country's renewable energy resources and fossil fuel energy generations. \n",
    "\n",
    "We however advised that the government should continue to pursue its target of 74% sizeable buildout of new renewables capacity notably from wind and solar by year 2030. As such, a stable long term renumeration framework for supporting the growth of renewables, including storage will be essential.\n",
    "\n",
    "The model developed has the capacity to be deployed worldwide, most especially the developing countries in Africa where most of the challenges related to electricity include the need to reduce greenhouse gas emmissions, improve the energy efficiency and increase access to energy. \n",
    "\n",
    "The model if deployed in Nigeria, it will greatly reduce the most of the challenges of aging infrasctructure, changing consumer preferences, and the need to integrate new technologies.\n",
    "\n",
    "Renewable energy is growing rapidly worldwide, which is expected to continue to grow in the coming years, driven by falling costs and supportive government policies. It has accounted for 29% of global electricity generation in 2020, up from 27% in 2019.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
